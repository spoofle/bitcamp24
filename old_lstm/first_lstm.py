# -*- coding: utf-8 -*-
"""regression_application.ipynbl

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/199hELuqDJsVDeRBOUv8cp631Q4gH0aad
"""

import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense

# need to load database
from google.colab import drive
drive.mount('/content/drive')
loan_data = pd.read_csv('/content/drive/My Drive/Other/atlanta_financial.csv')

initial_interest = loan_data["Original_Interest_Rate"]
current_interest = loan_data["Current_Interest_Rate"]
print(initial_interest)
print(current_interest)

data = loan_data[['Original_Interest_Rate', 'Current_Interest_Rate']]

interest_data = np.column_stack((initial_interest, current_interest))

# Define sequence length and input features
sequence_length = 10
input_features = 2  # Since we have two features: initial interest rate and current interest rate

# Define X and y
X = []
y = []

# Create sequences with corresponding labels
for i in range(len(interest_data) - sequence_length):
    X.append(interest_data[i:i+sequence_length])
    y.append(interest_data[i+sequence_length, 1])  # Assuming you want to predict the current interest rate

# Convert lists to numpy arrays
X = np.array(X)
y = np.array(y)

# Split the data into training and testing sets
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# LSTM model
model = Sequential([
    LSTM(50, input_shape=(sequence_length, input_features)),
    Dense(1, activation='linear')  # Changed activation to 'linear' for regression
])

# Model compilation
model.compile(optimizer='adam', loss='mean_squared_error')  # Changed loss function for regression

# Model training
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Model evaluation
loss = model.evaluate(X_test, y_test)
print(f'Loss: {loss}')

"""Results of running the epochs:
Epoch 1/10
22269/22269 [==============================] - 169s 7ms/step - loss: 0.4684 - val_loss: 0.4422
Epoch 2/10
22269/22269 [==============================] - 168s 8ms/step - loss: 0.4458 - val_loss: 0.4430
Epoch 3/10
22269/22269 [==============================] - 170s 8ms/step - loss: 0.4457 - val_loss: 0.4424
Epoch 4/10
22269/22269 [==============================] - 168s 8ms/step - loss: 0.4456 - val_loss: 0.4453
Epoch 5/10
22269/22269 [==============================] - 170s 8ms/step - loss: 0.4457 - val_loss: 0.4423
Epoch 6/10
22269/22269 [==============================] - 163s 7ms/step - loss: 0.4455 - val_loss: 0.4450
Epoch 7/10
22269/22269 [==============================] - 170s 8ms/step - loss: 0.4455 - val_loss: 0.4470
Epoch 8/10
22269/22269 [==============================] - 162s 7ms/step - loss: 0.4454 - val_loss: 0.4420
Epoch 9/10
22269/22269 [==============================] - 162s 7ms/step - loss: 0.4454 - val_loss: 0.4448
Epoch 10/10
22269/22269 [==============================] - 177s 8ms/step - loss: 0.4454 - val_loss: 0.4446
5568/5568 [==============================] - 18s 3ms/step - loss: 0.4446
Loss: 0.44457313418388367
"""